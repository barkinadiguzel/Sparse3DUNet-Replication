# ðŸŽ† Sparse3DUNet-Replication â€” Volumetric Segmentation from Sparse Annotation

This repository provides a **PyTorch-based research replication** of  
**3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation â€” Ã‡iÃ§ek et al., 2016**,  
reproduced as a **theory-driven volumetric segmentation framework**.

The project translates the paperâ€™s **3D encoderâ€“decoder architecture, sparse supervision strategy, and elastic deformation model**
into a clean, modular research codebase.

- Enables **dense 3D segmentation from sparse 2D slice annotations** ðŸ§Š  
- Implements **3D convolutional U-Net with multi-scale context aggregation** ðŸ§   
- Integrates **elastic volumetric deformation for data augmentation** ðŸŒ€  

**Paper reference:**  
[3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation â€” Ã‡iÃ§ek et al., 2016](https://arxiv.org/abs/1606.06650) ðŸ“„

---

## ðŸŒŒ Overview â€” Sparse Volumetric Segmentation Pipeline

![3D U-Net Overview](images/figmix.jpg)

The core idea:

> Dense 3D understanding can emerge from sparse 2D supervision.

Instead of requiring full voxel-wise annotation:

$$
V \longrightarrow Y
$$

we train the model from **partially labeled slices**:

$$
V \xrightarrow{\text{sparse}} \hat{Y}
$$

where the learning objective is restricted to annotated voxels only.

The network learns a volumetric mapping

$$
f_\theta : \mathbb{R}^{D \times H \times W \times C} \rightarrow \mathbb{R}^{D \times H \times W \times K}
$$

producing a dense segmentation volume $\hat{Y}$ from a sparsely annotated input $V$.

---

## ðŸ”¬ Mathematical Formulation

Let the input volume be

$$
V \in \mathbb{R}^{D \times H \times W \times C}
$$

where  
- $D, H, W$ denote spatial dimensions  
- $C$ is the number of imaging channels  

The 3D U-Net learns a voxel-wise classifier:

$$
p(y_{ijk} \mid V) = \text{Softmax}(f_\theta(V)_{ijk})
$$

Training is performed using a **weighted sparse cross-entropy loss**:

$$
\mathcal{L} = - \sum_{i,j,k} w_{ijk} \sum_{c=1}^{K} y_{ijk}^c \log p_{ijk}^c
$$

where  

- $w_{ijk} = 0$ for unlabeled voxels  
- $w_{ijk} > 0$ for annotated slices  
- $K$ is the number of segmentation classes  

This allows learning from only a small subset of labeled slices while generalizing to the full volume.

---

## ðŸ§  What the Model Learns

- To aggregate **multi-scale 3D context** across volumetric structures ðŸ§­  
- To infer dense segmentation from sparse slice supervision ðŸ§©  
- To model anatomical continuity across adjacent slices ðŸ§¬  
- To remain robust under elastic volumetric deformations ðŸŒ€  
- To learn volumetric representations from extremely limited annotation ðŸ“‰  

The segmentation process becomes a **context-driven volumetric inference problem** rather than a slice-wise prediction task.

---

## ðŸ“¦ Repository Structure

```bash
Sparse3DUNet-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ unet3d.py                 # 3D U-Net architecture (paper replication)
â”‚   â”‚
â”‚   â”œâ”€â”€ loss/
â”‚   â”‚   â””â”€â”€ weighted_softmax.py      # Sparse-aware weighted loss
â”‚   â”‚
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ sparse_mask.py           # Slice-based sparse annotation handler
â”‚   â”‚   â””â”€â”€ augmentation.py          # Elastic deformation, rotation, intensity
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ forward_pipeline.py      # Volume â†’ Augment â†’ Tiles â†’ U-Net â†’ Output
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ visualization.py         # Slice & volume visualization
â”‚   â”‚
â”‚   â””â”€â”€ config.py                   # Patch size, num_classes, deformation params
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ figmix.jpg                     
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ðŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
